9 // item status
# Parallelism and Concurrency
For shared-memory scenarios, the *OpenMP* `pragma`-based interface for C++ allows a straightforward "high-level" parallelization of many prominent use cases for parallelization (.e.g, nested for loops) and also provides mechanisms to implement synchronization between parallel running threads (e.g., critical regions or atomic updates). 
OpenMP implementations typically come along with a compiler and support a certain version of the OpenMP standard.
A prominent alternative is `TBB` which supports similar use cases but is shipped as a thrid-party library (i.e., the integration is not `pragma`-based).

Currently, the C++ standard library provides support for threads and some synchronization primitives. Additionally, `std::atomic` provides a wrapper for atomic types with specialization for integers and pointers.

We will have a look at some important synchronization primitives to illustrate "how much batteries are included" in the standard library w.r.t. parallelism and concurrency.

## std::thread
Constructing a `std::thread` in C++ can look likes this, when using a callable which requires some arguments:
```pmans
      auto callable = [](int a, int b) {
        std::cout << a + b << std::endl;
        return a + b;
      };
      int arg1 = 1;
      int arg2 = 1;
      std::thread thread(callable, arg1, arg2);
      thread.join();
``` 
Here, a function object obtained from a lambda expression is used.
After construction `thread` immediately invokes the callable using the provided arguments in a new thread of execution:
- no local variables are available
- global variables are accessible

> Can variables be made available in the new thread when capturing them as references or by-value?

The construction of a thread does not support passing references as constructor arguments, this is why the following is not immediately possible:
```pmans
      auto callable = [](int &a, int &b) { ... };
      int arg1 = 2;
      int arg2 = 2;
      std::thread thread(callable, arg1, arg2); // does not compile
      ...
```
> How to overcome this problem if we want to pass a reference (e.g., a large object to be manipulated by the thread)?

As we have seen above, a std::thread requires an explicit `.join()` before the application ends. 
A lightweight wrapper can be used if desired to automatically join the thread when the variable is destructed:
```pmans
struct jthread {
  std::thread t;
  template <class... Args>
  explicit jthread(Args &&... args) : t(std::forward<Args>(args)...) {}
  ~jthread() { t.join(); }
};
```
Up to now we saw how to create threads which execute a provided callable but we did not really care about the returned value of the callable.

```
clang++ -std=c++17 thread.cpp -O3 -pthread && ./a.out
```

## std::future and std::promise
The approach to conveniently observe and obtain return values of callables executed in an another thread provided by the standard library are `std::promise` and `std::future`.
Let's see an example which does not even invovle different threads:
```pmans
  auto promise = std::promise<int>();
  auto future = promise.get_future();
  {
    auto status = future.wait_for(std::chrono::milliseconds(1));
    assert(std::future_status::timeout == status);
  }
  promise.set_value(2);
  {
    auto status = future.wait_for(std::chrono::milliseconds(1));
    assert(std::future_status::ready == status);
    future.wait(); // blocking
    auto value = future.get(); // get 2
  }
```
> What is the basic idea of std::promise/std::future pair, how might an implementation loop like?

Now let's see the same example when using a thread to set "fullfill the promise":
```pmans
  auto promise = std::promise<int>();
  auto future = promise.get_future();
  auto callable = [&promise]() {
    std::this_thread::sleep_for(std::chrono::milliseconds(10));
    promise.set_value(4);
  };
  std::thread t(std::move(callable));
  {
    auto status = future.wait_for(std::chrono::milliseconds(1));
    assert(std::future_status::timeout == status);
  }
  {
    future.wait();             // blocking
    auto value = future.get(); // get 4
    // auto value2 = future.get(); // exception
  }
  t.join();
```
> We can see that the callable had to be adopted (compared to having a regular return value). Is this desireable?

A convenient approch to untilize "unmodified" callables with non void return types with threads is `std::packaged_task`:
```pmans
  auto callable = []() {
    return 6;
  };
  auto task = std::packaged_task<int()>(std::move(callable));
  auto future = task.get_future();
  std::thread t(std::move(task));
  {
    future.wait();             // blocking
    auto value = future.get(); // get 6
    std::cout << value << std::endl;
  }
  t.join();
```
We can see that a callable object can stay unmodifed when executed by another thread.

```
clang++ -std=c++17 futures.cpp -O3 -pthread && ./a.out
clang++ -std=c++17 thread.cpp -O3 -pthread && ./a.out  # (last item)
```

## std::async
To even further simplify the triggering of a execution of a callable in a separate thread `std::async` can be used:
```pmans
  auto callable = [](int N, const std::string &str) {
    for (int i = 0; i < N; ++i)
      std::cout << str << std::endl;
  };
  int arg1 = 3;
  auto a1 = std::async(callable, arg1, "default");
  auto a2 = std::async(std::launch::deferred, callable, arg1, "deferred");
  auto a3 = std::async(std::launch::async, callable, arg1, "async");
  auto a4 = std::async(std::launch::async, callable, arg1, "async2");
  f4.wait();
  f3.wait();
  f1.wait();
  f2.wait();  
```
> What can we expect in terms of asynchronicity w.r.t to the different launch policies `async` and `deferred`?

Also `std::async` exhibits some properties which might be unexpected:

**Example #1**
```pmans
  {
    auto future1 = std::async(std::launch::async, callable, arg1, "async");
  }
  {
    auto future2 = std::async(std::launch::async, callable, arg1, "async");
  }  
```
**Example #2**
```pmans
  { // temporay object (future) block on destruction
    std::async(callable, arg1, "is this ...");
    std::async(callable, arg1, "... async?");
  }
```
> For the two example above, will the two calls result in an overlapping execution of `callable` in two threads?

```
clang++ -std=c++17 async.cpp -O3 -pthread && ./a.out
```

## Locking
For the probably most common synchronization task, i.e., protecting read or write access to a shared variable, the standard library provides `std::mutex` which is recommended to be used only in conjuntion with a `std::unique_lock`  or `std::lock_guard`.
If a mutex would be used without a lock this can look like this:
```pmans
    std::mutex m;
    std::vector<double> shared_data;
    auto manip = [&m, &shared_data]() {
      m.lock();
      // manipulate shared_data
      m.unlock();
    };
```
> Why is this useage error-phrone?

When unsing a `lock_guard` the example transforms to this:
```pmans
    std::mutex m;
    std::vector<double> shared_data;
    auto manip = [&m, &shared_data]() {
      std::lock_guard<std::mutex> lock(m);
      // manipulate shared_data
    };
```
In situations where is is required to aquire multiple mutexed before performing a manipulation, `unique_lock` can be utilized like this:
```pmans
    std::mutex m1;
    std::mutex m2;
    std::vector<double> shared_data1;
    std::vector<double> shared_data2;
    auto manip = [&m1, &m2, &shared_data1, &shared_data2]() {
      std::unique_lock<std::mutex> dlock1(m1, std::defer_lock);
      std::unique_lock<std::mutex> dlock2(m1, std::defer_lock);
      std::lock(dlock1, dlock2); 
      // manipulate shared_data1 and shared_data2 together
    };
```
> Why is the last snippet preverable over a sequential locking using two `lock_guards`?

Locking is in no way a lightweight approach: only a single thread can execute the locked region and all other threads are blocked on entry.
> Let's look a performance comparison without even using more than one thread:
```pmans
    std::vector<int> vec(N, 1.0);
    int sum = 0;
    auto accumulate = [&sum, &vec]() {
      for (auto &&item : vec) {
        sum = sum + 1; // critical region: benchmark std::atomic vs lock_guard vs no synchronization
      }
    };
```

```
clang++ -std=c++17 serial_atomic_vs_lock.cpp -O3  && ./a.out 
```

Let's further move to an example where the synchronization is actually required because multiple threads are involved:
```pmans
struct Other {
  int a = 5;
  int b = 5;
}; // a+b is always 10;

struct Widget { 
  Other o;
  void mod1() {
    if (o.a > 0) {
      --o.a;
      ++o.b;
    }
  }
  void mod2() {
    if (o.a > 0) {
      ++o.a;
      --o.b;
    }
  }
  int inspect() const { return o.a + o.b; }
};
```
> We will lock how multithreaded accesses `Widget` above can be synchronized to guarantee the invariant of `Other`, namely `a+b==10`
```
clang++ -std=c++17 mutex_lock.cpp -O3 -pthread && ./a.out
```

## std::condition_variable
Another important synchronization primitive in the standard library is `std::condition_variable`: it allows to suspend the execution of threads an notify a single or all of them if a condition becomes true. This can be used to avoid busy wainting of existing threads which have completed their tasks and shall be reused once new tasks are available.
> Why can it be attractive to reuse threads for subsequent tasks?

The `std::condition_variable` is always used in combination with a lock, let's seen a minimal example to demonstrate it's usefullness:
```pmans
clang++ -std=c++17 convar.cpp -O3 -pthread && ./a.out 
```

## A thread pool
Finally, we will lock at a simple thread pool implementation building on top of all previously discusses primitives:
- `std::async` + `std::future`: for launching the initial threads and to obtain a future as handles
- `std::condition_variable`: to notify one the worker threads when a task/work-item is ready to be assigned
- `std::unique_lock`: to guard the multithreaded access to the queue of work items
- `std::condition_variable`: to notify all threads about the shut down of the thread pool

```pmans
clang++ -std=c++17 thread_pool.cpp -O3 -pthread && ./a.out 
```

## std::atomic
The standard library provides a wrapper for synchonizing access to entities which exhibit native support for atomic operations:
- integer types
- pointer types
```pmans
std::atomic<int> a(0);
a++;            // perform atomic increment (specialization for int)
a.fetch_add(1); // equivalent
a += 5;         // perform atomic addition (specialization for int)
a.fetch_add(5); // equivalent
```

> Is the expression `a = a + 5;` below atomic as a whole?
```pmans
std::atomic<int> a(0);
a = a + 5;
```

Atomics can be used for the variable which is shared directly, but they can also be used as index to non-atomic memory, see below for a simplisitc example:
```pmans
int queue[N]:
std::atomic<size_t> front;
void push(int x) {
    size_t unique_index = front.fetch_add(1);
    queue[unique_index] = x; // unique access here, no interference
}
```
Lock-free implemetations of data structures follow this scheme, see link in the references.


## Links
- std::thread https://en.cppreference.com/w/cpp/thread/thread
- std::promise https://en.cppreference.com/w/cpp/thread/promise
- std::future https://en.cppreference.com/w/cpp/thread/promise
- std::async https://en.cppreference.com/w/cpp/thread/async
- std::mutex https://en.cppreference.com/w/cpp/thread/mutex
- std::unique_lock https://en.cppreference.com/w/cpp/thread/unique_lock
- std::lock_guard https://en.cppreference.com/w/cpp/thread/lock_guard
- std::atomic https://en.cppreference.com/w/cpp/atomic/atomic
- Talk on `std::atomics` from cppcon2017 https://www.youtube.com/watch?v=ZQFzMfHIxng 
